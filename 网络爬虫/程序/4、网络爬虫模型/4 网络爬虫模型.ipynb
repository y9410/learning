{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56bc943",
   "metadata": {},
   "source": [
    "# 规划和定义对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efde8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#当决定抓取哪些数据时，最好的做法是忽视所有的网站\n",
    "#当你启动一个可扩展的大型项目时，不是首先查看单个网站并且问“存在什么？”，而是要自问“我需要什么？”，然后想方设法从中寻找所需信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d62437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可能你真正需要做的就是比较多个商店的产品价格，并且追踪这些价格的变化\n",
    "#这种情况下，你需要足够的信息来唯一地识别各个产品，就是这么简单\n",
    "#产品名称、制造商、产品 ID 号（如果可以获得或者相关的话）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469641c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#以上这些信息并不特定于某一商店。例如，产品评论、评分、价格，甚至描述都是针对特定商店的特定产品的\n",
    "#这些信息可以单独保存。其他信息（产品的颜色、材质）是特定于产品的，但是可能很稀疏，因为并不是所有产品都有这个信息\n",
    "#所以我们需要后退一步，对你考虑的每一项都做一个清单检查，然后问自己以下几个问题\n",
    "#1、这个信息可以帮助项目实现目标吗？如果我不包括该信息，是否会造成阻碍？还是说该信息有了固然好，但是并不会影响任何结果？\n",
    "#2、如果该信息将来可能有帮助，但是我并不确定，那么晚些时候再抓取会有多大的困难？\n",
    "#3、这个数据对于我已经抓取的信息来说是否冗余？\n",
    "#4、将数据存储在这个对象中是否符合逻辑？（正如前面提到的，如果同一产品在不同网站上的描述不一致的话，那么存储该产品的描述信息就没有意义。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ab0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果你确定需要抓取该数据，那么就要问自己以下问题，然后确定如何在代码中存储并处理这些数据\n",
    "#1、该数据是稀疏的还是密集的？它与每个清单都相关并且会出现在其中，还是只与部分清单相关？\n",
    "#2、该数据有多大？\n",
    "#3、在数据较大的情况下，我每次运行分析时都需要检索该数据，还是只是偶尔需要使用该数据？\n",
    "#4、 这种类型的数据有多大的变化性？我需要经常加入新的属性、修改类型（例如面料样式可能是经常修改的属性）吗？还是说该数据一直保持不变（鞋的码数）？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0e15e",
   "metadata": {},
   "source": [
    "# 处理不同的网站布局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3820ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e48d1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = '10.204.32.132:7890'\n",
    "proxies = {\"http\": \"http://%(proxy)s/\" % {'proxy': proxy}, \"https\": \"http://%(proxy)s/\" % {'proxy': proxy}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eca927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个 Content 类的示例（代表网站上的一块内容，如新闻文章）\n",
    "#其中两个抓取器函数以 BeautifulSoup 对象作为输入，返回一个 Content 实例\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "def getPage(url, proxies = proxies):\n",
    "    req = requests.get(url, proxies = proxies, verify=False)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    lines = bs.find_all(\"p\", {\"class\":\"story-content\"})\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    body = bs.find(\"div\",{\"class\",\"post-body\"}).text\n",
    "    return Content(url, title, body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cbfda91",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Delivering inclusive urban access: 3 uncomfortable truths\n",
      "URL: https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\n",
      "\n",
      "\n",
      "The past few decades have been filled with a deep optimism about the role of cities and suburbs across the world. These engines of economic growth host a majority of world population, are major drivers of economic innovation, and have created pathways to opportunities for untold amounts of people.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jeffrey Gutman\n",
      "\n",
      "\t\t\t\t\tFormer Nonresident Fellow, Global Economy and Development\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Adie Tomer\n",
      "\n",
      "\t\t\t\t\tSenior Fellow - Brookings Metro \n",
      "\n",
      " Twitter\n",
      "AdieTomer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "But all is not well within our so-called Urban Century. Rapid urbanization, rising gentrification, concentrated poverty, and shortages of basic infrastructure have combined to create spatial inequity in cities and suburbs across the globe. The challenges of housing, moving, and employing so many people have led to longer travel times, rising housing costs, and unsustainable public spending. Moreover, policymakers are questioning traditional policies and approaches.\n",
      "The past couple years, we’ve led a project at Brookings—Moving to Access—that responds to these spatial challenges by promoting the idea of connecting people to opportunities as a new foundational principle for 21st century urban development. This principle of accessibility is meant to be a corollary to the natural questions we ask ourselves everyday about the communities where we live: Is this the best location to access employment? Are there nearby schools and health services? Is there a market in the neighborhood? How can I get from here to there? Such choices are valid for those with sufficient income. But what about those with more limited resources and thus choices in terms of affordable housing and affordable transport?\n",
      "While economists, planners, and engineers have promoted accessibility for decades, the concept is more often found in textbooks than formal urban policies. In the first stage of this project, we worked with a team of experts to determine what has stalled practical implementation of appropriate policies and practices? “Delivering Inclusive Access,” a report of this initial work, offers a synthesis of what we found and where we believe researchers, policymakers, and practitioners can take this work next. The paper found three central challenges.\n",
      "The fallacy of the single indicator\n",
      "The current transport regime’s approach to measurement is one of outward elegance: The dominant pursuit is speed, and the primary way to measure it is congestion (or what slows us down). Many have come to label this approach a pursuit of “mobility.” It is seen through different, but often singular, measures of how congestion affects a specific roadway. Such singular measures are easily interpreted by policymakers and civil society and can be translated directly into economic analysis of related investments through timesavings. They also conveniently serve such purposes as the internationally agreed-upon Sustainable Development Goals. Yet they actually don’t answer the fundamental question of who can reach where, in how much time, and at what cost.\n",
      "\n",
      "\n",
      "Related Content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Delivering inclusive access\n",
      "\n",
      "Jeffrey Gutman, Adie Tomer, Joseph W. Kane, Nirav Patel, and Ranjitha Shivaram\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Measuring performance: Accessibility metrics in metropolitan regions around the world\n",
      "\n",
      "Geneviève Boisjoly and Ahmed El-Geneidy\n",
      "August 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Report\n",
      "Is better access key to inclusive cities?\n",
      "\n",
      "Jeffrey Gutman and Nirav Patel\n",
      "Wednesday, October 5, 2016\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessibility measures can answer those questions, but not through any one measure. First, the variable social, economic, and political contexts related to access mean searching for a single magical indicator is counterintuitive. For example, a wealthy, automobile-centric region like Dallas, Texas, may have very different measurable goals than a denser, poorer region like Dar es Salaam, Tanzania. Second, academic literature is now rife with such complex measures that it could be difficult to communicate their methodology and results with practitioners. The development of a suite of indicators could offer a menu for policymakers and practitioners to judge accessibility based on local objectives, local conditions, and local capacity.\n",
      "The danger of excessive localization\n",
      "Decentralization and empowering local communities is fast becoming a mantra of governance experts across the world, from development practitioners at institutions like the World Bank to city-focused theorists. And for good reason: delegating policy design and fiscal authority directly to the local level helps ensure policies and practices respond to local needs and desires. Yet as urban areas spillover into contiguous and often numerous municipalities, local independence can introduce certain challenges, especially relating to social and environmental externalities. When it comes to transportation and land development, interests of one municipality are often different from its neighbors. And these divergent development goals can exacerbate accessibility challenges within growing regions, spreading people, housing jobs, and other activities further from one another.\n",
      "Addressing spatial inequities in land use and real estate markets require a broader approach to horizontal governance. While there are examples of metropolitan transport authorities, there is less willingness to consider metropolitan or horizontal governance of land use and fiscal policies. For example, should housing be coordinated across an entire region?\n",
      "Countries with a more centralized top down approach to governance, such as France and Germany, have greater ability to formulate metropolitan governance than more decentralized countries such as the U.S. This is not to say there is a one-size-fits-all approach, but there is an opportunity to test different solutions within different governance contexts, comparing how effective each model is to promote spatial inclusivity.\n",
      "The finance community is missing in action\n",
      "Financing is a central topic in infrastructure circles. As maintenance bills from the automobile era come due, populations continue to grow, and fiscal budgets are tight, how can urban areas afford to build enough infrastructure to support future economic growth? In response, new approaches are evolving in fiscal instruments, such as value capture and private-public partnerships. Missing in these discussions, however, are the implications for inclusive access.\n",
      "We conducted a multi-decade review of past academic literature on access and found that there is no clear substantive discussion of accessibility from a fiscal perspective. While urban transport and land use professionals clearly recognize their interrelationship in achieving inclusive accessibility, at least in theory, the fiscal and finance professionals generally ignore the implications of their instruments with regard to inclusivity. The multilateral development banks and their economic evaluations have ignored the distributive impacts until very recently. And the efforts of some countries to incorporate measures through multi-criteria analysis have had limited impact.\n",
      "This gap must be resolved in any effort toward inclusive urban development. There is little doubt that fiscal approaches must carefully assess who ultimately pays and that alternative finance instruments should be adapted to foster access for all.\n",
      "Going forward\n",
      "Our research confirms that there are enormous opportunities to advance accessibility theory into practice. At this point, what is desperately needed is to launch a range of case studies that deal with these issues and challenges under different geographic, governance, and economic contexts. The good news is that many initiatives are already underway, and more robust communication channels and technology can support such efforts. In Chicago, researchers created an online platform to visually explore accessibility by location. In Bogota, researchers evaluated how affordability is a key principle of access. And in Cairo and Kigali, researchers used open tools to achieve new insights for accessibility. Sharing the results of these case studies could lead to a new level of cross-disciplinary approaches to improve accessibility and lessen the effects of spatial inequity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b9f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#当你为额外的新闻网站添加抓取器函数时，可能会发现存在一种模式。每个网站的解析函数基本上在做同样的事情：\n",
    "#1、选择标题元素并从标题中抽取文本\n",
    "#2、选择文章的主要内容\n",
    "#3、按需选择其他内容项\n",
    "#4、返回此前由字符串实例化的 Content 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab415a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    所有文章/网页的共同基类\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        用灵活的打印函数控制结果\n",
    "        \"\"\"\n",
    "        print(\"URL: {}\".format(self.url))\n",
    "        print(\"TITLE: {}\".format(self.title))\n",
    "        print(\"BODY:\\n{}\".format(self.body))\n",
    "        \n",
    "# Website 类并不存储任何从页面本身抓取的信息，而是存储关于如何抓取数据的指令\n",
    "#它也不存储“My Page Title”这样的标题。它只会存储字符串标签 h1，表明了在哪里可以找到标题\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    描述网站结构的信息\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad5e2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个爬虫，可以抓取任何网站的任何网页的标题和内容\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url, proxies = proxies):\n",
    "        try:\n",
    "            req = requests.get(url, proxies = proxies, verify=False)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def safeGet(self, pageObj, selector):\n",
    "        \"\"\"\n",
    "        用于从一个BeautifulSoup对象和一个选择器获取内容的辅助函数。\n",
    "        如果选择器没有找到对象，就返回空字符串\n",
    "        \"\"\"\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "    \n",
    "    def parse(self, site, url):\n",
    "        \"\"\"\n",
    "        从指定URL提取内容\n",
    "        \"\"\"\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "        if title != '' and body != '':\n",
    "            content = Content(url, title, body)\n",
    "            content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f456a81a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
      "TITLE: Idea to Retire: Old methods of policy education\n",
      "Idea to Retire: Old methods of policy education\n",
      "BODY:\n",
      "\n",
      "Public policy and public affairs schools aim to train competent creators and implementers of government policy. While drawing on the principles that gird our economic and political systems to provide a well-rounded education, like law schools and business schools, policy schools provide professional training. They are quite distinct from graduate programs in political science or economics which aim to train the next generation of academics. As professional training programs, they add value by imparting both the skills which are relevant to current employers, and skills which we know will be relevant as organizations and societies evolve. \n",
      "The relevance of the skills that policy programs impart to address problems of today and tomorrow bears further discussion. We are living through an era in which societies are increasingly interconnected. The wide-scale adoption of devices such as the smartphone is having a profound impact on our culture, communities, and economy. The use of social and digital media and associated means of communication enabled by mobile devices is changing the tone, content, and geographic scope of our conversations, modifying how information is generated and consumed, and changing the very nature of citizen engagement. \n",
      "Information technology-based platforms provisioned by private providers such as Facebook, Google, Uber, and Lyft maintain information about millions of citizens and enable services such as transportation that were mediated in the past solely by the public sector. Surveillance for purposes of public safety via large-scale deployment of sensors also raises fundamental questions about information privacy. From technology-enabled global delivery of work to displacement and replacement of categories of work, some studies estimate that up to 47 percent of U.S. employment might be at risk of computerization with an attendant rise in income inequality. These technology-induced changes will affect every policy domain. How should policy programs best prepare students to address societal challenges in this world that is being transformed by technology? We believe the answer lies in educating students to be “men and women of intelligent action.” \n",
      "A model of policy education\n",
      "We begin with a skills-based model of policy education. These four essential skills address the general problems policy practitioners frequently face:\n",
      "\n",
      "Design skills to craft policy ideas \n",
      "Analytical skills to make smart ex ante decisions \n",
      "Interpersonal experience to manage policy implementation  \n",
      "Evaluative skills to assess outcomes ex post and correct course if necessary\n",
      "\n",
      "These skills make up the policy analysis toolkit required to be data driven practitioner of “intelligent action” in any policy domain. This toolkit needs to be supplemented by an understanding of how technology is transforming societal challenges, enabling new solutions, or disrupting existing regulatory regimes. This understanding is essential to policy formulation and implementation. \n",
      "Pillar 1: Design skills\n",
      "As with engineering, where design precedes analysis, this first pillar seeks to educate students in thinking creatively about problems in order to devise and develop policy ideas. Using ideas derived from design, divergent and convergent thinking principles are employed to generate, explore, and arrive at a candidate set of solutions. Using Uber as an example, an approach to identify and explore the key policy issues such as convenience, costs, driver working hours, and insurance would involve interviewing and observing both incumbent taxi drivers and Uber drivers. This in turn would lead to a set of alternatives that deserve further and careful consideration.  Using these skills, candidate designs and choices that are generated can be evaluated using the policy analytic toolkit. \n",
      "Pillar 2: Analytical skills\n",
      "At Carnegie Mellon, we are often cited in media and interrogated by peers on our approach to analytical and technology skills education. Curiosity about which skills are the “right” skills to teach policy practitioners are common, but we believe this is the wrong approach. We instead begin from the premise that policy or management decisions should be grounded in evidence.  We then determine the skills required to assemble the types of evidence that will likely be available to policy makers in the future.  In increasingly instrumented environments where citizens and infrastructure produce continuous streams of data, making sense of it all will require a somewhat different set of skills. We believe that a grounding in micro-economics, operations research, statistics, and program evaluation (aka causal inference) to be an essential core to policy programs. \n",
      "New coursework will teach students to work with multi-variable data and machine learning with an emphasis on prediction. This material ought to be part of the required coursework in statistics given the importance of prediction in many policy implementation settings. Along the same lines, the ability to work with unstructured data (especially text) and data visualization will become increasingly relevant to all students, not just those students who want to specialize in data analytics. Finally, knowledge of data manipulation and analysis languages such as Python and R for analytic work will be important because data often has to be massaged and cleansed prior to analysis. An important task for programs will be to determine the competencies expected of graduates. \n",
      "Pillar 3: Interpersonal experiences\n",
      "The third pillar of the skills-based model is interpersonal experience, where the practiced habits of good communication and steady negotiation developed with a sound understanding of organizations, their design and their behaviors. We label these purposely as experiences rather than skills because we believe they are best practiced either in the real-world or in simulated real-world settings. It is also in this pillar where practitioners learn the knowledge necessary to become credible experts in their domain. We believe that in addition to core coursework in the area, a supplementary curriculum which provides students with opportunities to gain these experiences is an essential component of our educational model.\n",
      "Pillar 4: Evaluative skills\n",
      "\n",
      "\n",
      "Related Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Government Policy toward Open Source Software\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tEdited by Robert W. Hahn \n",
      "2002\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Broadband\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tEdited by Robert W. Crandall and James H. Alleman \n",
      "2003\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "The Need for Speed\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tBy Robert E. Litan and Hal J. Singer \n",
      "2013\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The ability to carefully diagnose the effectiveness of policy or management interventions is the fourth pillar of our model. It is insufficient to create and execute policy without measurement, and this is where both careful thought to the fundamental issues of measurement and evaluation become important. The ability to make objective judgments on the benefits, liabilities, and unintended consequences of prior policies is the goal of this set of skills. Here, sound statistical and econometric training with an understanding of the principles of causal inference is essential. In addition, program evaluation skills such as cost-benefit and financial analysis help practitioners round out their evaluation skills by considering both non-monetary and economic impacts.\n",
      "What should be retired?\n",
      "A skills-based approach might replace certain aspects of existing policy training.  This depends on a number of factors specific to each institution, but three generally applicable observations are clear. First, real-world experiences are a powerful way to encode domain learning as well as project management skills. Through project-based work, students can learn about institutional contexts in specific policy domains and political processes such as budgeting. Second, team-based projects allow students to learn and apply principles of management and organizational behavior. At Carnegie Mellon, we refer to these as “systems synthesis” projects, since they require students to adopt a systemic point of view and to synthesize a number of skills in their policy analysis toolkit. Third, interpersonal skills training can be practiced through activities such as weekend negotiation exercises, hackathons, and speaker series. These activities can be highly intentional and fashioned to reinforce skills rather than as a recess from the “real work” of classroom training. Since students complete graduate programs in such a short time, counseling them to focus on outcomes from day one will allow them to choose a reinforcing set of coursework and real-world experiences. \n",
      "In summary, we argue for a model of policy education that views practitioners as future problem solvers. Good policy education must consider the ways in which problems will present themselves, and the ways in which answers will obscure themselves. Rigorous training grounded in the analysis of available evidence and buoyed by real-world interpersonal experiences is a sound approach to relevant, durable policy training.\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "\n",
      "Ramayya Krishnan\n",
      "\n",
      "\t\t\t\t\tRamayya Krishnan is the dean of H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University where he is the W.W. Cooper and Ruth F. Cooper Professor of Management Science and Information Systems.\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "J\n",
      "\n",
      "\n",
      "\n",
      "Jon Nehlsen\n",
      "\n",
      "\t\t\t\t\tJon Nehlsen is senior director of external relations at H. John Heinz III College of Information Systems and Public Policy at Carnegie Mellon University.\t\t\t\t\t\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Read other essays in the Ideas to Retire blog series here.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "#以下代码定义了网站对象并开启了流程\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [['O\\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'],\n",
    "            ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "            ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "            ['New York Times', 'http://nytimes.com', 'h1', 'p.story-content']]\n",
    "\n",
    "websites = []\n",
    "\n",
    "for row in siteData:\n",
    "    websites.append(Website(row[0], row[1], row[2], row[3]))\n",
    "\n",
    "crawler.parse(websites[0], 'https://shop.oreilly.com/product/')\n",
    "crawler.parse(websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(websites[2], 'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "crawler.parse(websites[3], 'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295c811",
   "metadata": {},
   "source": [
    "# 结构化爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d92020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过搜索抓取网站"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8b419d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#搜索存在几个特点\n",
    "#1、大多数网站通过将主题作为参数在 URL 中传递，来获得特定主题的搜索结果列表。例如，http://example.com?search=myTopic。这个 URL 的第一部分可以存为 Website 对象的一个属性，简单地在其后添加主题\n",
    "#2、在搜索后，大多数网站以非常好识别的链接列表的形式呈现结果页面，通常会使用一个如 <span class=\"result\"> 的标签，而其准确的形式也可以存为 Website 对象的一个属性\n",
    "#3、每个结果链接要么是一个相对 URL（例如 /articles/page.html），要么是一个绝对 URL（例如 http://example.com/articles/page.html）。不管是相对 URL 还是绝对 URL，你都可以将其存为 Website 对象的一个属性\n",
    "#4、当定位到并规范化搜索页面的 URL 后，你就成功地将问题简化为上一节示例中的问题了——抽取给定格式网站的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6076f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    所有文章/网页的共同基类\n",
    "    \"\"\"\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        用灵活的打印函数控制结果\n",
    "        \"\"\"\n",
    "        print(\"New article found for topic: {}\".format(self.topic))\n",
    "        print(\"TITLE: {}\".format(self.title))\n",
    "        print(\"BODY:\\n{}\".format(self.body))\n",
    "        print(\"URL: {}\".format(self.url))\n",
    "        \n",
    "#程序中的 Website 类加入了一些新的属性\n",
    "#如果你附加了要搜索的主题，那么 searchUrl定义了可以在哪里获得搜索结果\n",
    "# resultListing 定义了存放每个结果信息的“盒子”（box）\n",
    "# resultUrl 定义了这个盒子中的标签，这些标签即为结果的准确 URL\n",
    "# absoluteUrl 属性是一个布尔值，它表示搜索结果是绝对 URL 还是相对 URL\n",
    "class Website:\n",
    "    \"\"\"描述网站结构的信息\"\"\"\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl=absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3eaafef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler 被进一步扩展，它包含 Website 数据、待搜索的主题列表和两个对所有这些网站和主题进行迭代的循环\n",
    "#它还包括一个 search 函数，该函数对特定网站和主题的搜索页面进行导航，并抽取页面中所有的结果 URL\n",
    "class Crawler:\n",
    "\n",
    "    def getPage(self, url, proxies = proxies):\n",
    "        try:\n",
    "            req = requests.get(url, proxies = proxies, verify=False)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    def safeGet(self, pageObj, selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return \"\"\n",
    "\n",
    "    def search(self, topic, site):\n",
    "        \"\"\"\n",
    "        根据主题搜索网站并记录所有找到的页面\n",
    "        \"\"\"\n",
    "        bs = self.getPage(site.searchUrl + topic)\n",
    "        searchResults = bs.select(site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(site.resultUrl)[0].attrs[\"href\"] #检查一下是相对URL还是绝对URL\n",
    "            if(site.absoluteUrl):\n",
    "                bs = self.getPage(url)\n",
    "            else:\n",
    "                bs = self.getPage(site.url + url)\n",
    "            if bs is None:\n",
    "                print(\"Something was wrong with that page or URL. Skipping!\")\n",
    "                return\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(topic, title, body, url)\n",
    "                content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a0c3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler()\n",
    "\n",
    "siteData = [['O\\'Reilly Media', 'https://oreilly.com',\n",
    "             'https://ssearch.oreilly.com/?q=','article.product-result',\n",
    "             'p.title a', True, 'h1', 'section#product-description'],\n",
    "            ['Reuters', 'https://reuters.com',\n",
    "             'https://www.reuters.com/search/news?blob=',\n",
    "             'div.search-result-content','h3.search-result-title a',\n",
    "             False, \n",
    "             'h1', \n",
    "             'div.StandardArticleBody_body_1gnLA'],\n",
    "            ['Brookings', 'https://www.brookings.edu',\n",
    "             'https://www.brookings.edu/search/?s=',\n",
    "             'div.list-content article', 'h4.title a',\n",
    "             True,\n",
    "             'h1',\n",
    "             'div.post-body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8206dd0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING INFO ABOUT: python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '10.204.32.132'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-a0c0141de009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GETTING INFO ABOUT: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtargetSite\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msites\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetSite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-ecdc07718dfb>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, topic, site)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Something was wrong with that page or URL. Skipping!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-ecdc07718dfb>\u001b[0m in \u001b[0;36mgetPage\u001b[1;34m(self, url, proxies)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequestException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    585\u001b[0m         }\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    497\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m                 )\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    698\u001b[0m             )\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhttp_tunnel_required\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    992\u001b[0m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtls_in_tls_required\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mssl_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         )\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m         )\n\u001b[0;32m    452\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         )\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m    868\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sites = []\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]))\n",
    "\n",
    "topics = ['python', 'data science']\n",
    "for topic in topics:\n",
    "    print(\"GETTING INFO ABOUT: \" + topic)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic, targetSite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9e70b",
   "metadata": {},
   "source": [
    "# 通过链接抓取网站"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55ad6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#该爬虫可以跟踪任意遵循特定 URL 模式的链接\n",
    "#这种爬虫非常适用于从一个网站抓取所有数据的项目，而不适用于从特定搜索结果或页面列表抓取数据的项目\n",
    "#它还非常适用于网站页面组织得很糟糕或者非常分散的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fae1b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这些类型的爬虫并不需要像上一节通过搜索页面进行抓取中采用的定位链接的结构化方法，因此在 Website 对象中不需要包含描述搜索页面的属性\n",
    "#由于爬虫并不知道待寻找的链接的位置，所以你需要一些规则来告诉它选择哪种页面\n",
    "#可以用 targetPattern（目标 URL 的正则表达式）和布尔变量 absoluteUrl 来达成这一目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b90ff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "559094f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.targetPattern = targetPattern\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "class Content:\n",
    "    \"\"\"\n",
    "    所有文章/网页的共同基类\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        用灵活的打印函数控制结果\n",
    "        \"\"\"\n",
    "        print(\"URL: {}\".format(self.url))\n",
    "        print(\"TITLE: {}\".format(self.title))\n",
    "        print(\"BODY:\\n{}\".format(self.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65cda099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler 类从每个网站的主页开始，定位内链，并解析在每个内链页面发现的内容\n",
    "class Crawler:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "        self.visited = []\n",
    "\n",
    "    def getPage(self, url, proxies = proxies):\n",
    "        try:\n",
    "            req = requests.get(url, proxies = proxies, verify=False)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    def safeGet(self, pageObj, selector):\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def parse(self, url):\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, self.site.titleTag)\n",
    "            body = self.safeGet(bs, self.site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()\n",
    "                \n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        获取网站主页的页面链接\n",
    "        \"\"\"\n",
    "        bs = self.getPage(self.site.url)\n",
    "        targetPages = bs.findAll('a',\n",
    "        href=re.compile(self.site.targetPattern))\n",
    "        for targetPage in targetPages:\n",
    "            targetPage = targetPage.attrs['href']\n",
    "            if targetPage not in self.visited:\n",
    "                self.visited.append(targetPage)\n",
    "                if not self.site.absoluteUrl:\n",
    "                    targetPage = '{}{}'.format(self.site.url, targetPage)\n",
    "                self.parse(targetPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea02a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#忽略requests证书警告\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5229beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = Website('Reuters', 'https://www.reuters.com', '^(/article/)', False, 'h1', 'div.StandardArticleBody_body_1gnLA')\n",
    "crawler = Crawler(reuters)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3fc8f",
   "metadata": {},
   "source": [
    "# 抓取多种类型的页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b28f3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#与抓取预定义好的页面集合不同，抓取一个网站的所有内链会带来一个挑战，即你不知道会获得什么。好在有几种基本的方法可以识别页面类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e290b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过URL\n",
    "#一个网站中所有的博客文章可能都会包含一个 URL（例如 http://example.com/blog/titleof-post）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "12f2b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过网站中存在或者缺失的特定字段\n",
    "#如果一个页面包含日期，但是不包含作者名字，那你可以将其归类为新闻稿\n",
    "#如果它有标题、主图片、价格，但是没有主要内容，那么它可能是一个产品页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af06aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过页面中出现的特定标签识别页面\n",
    "#即使不抓取某个标签内的数据，你仍然可以利用这个标签\n",
    "#你的爬虫可以寻找类似于 <div id=\"related-products\"> 这样的元素来识别产品页面，即便是爬虫对相关产品的内容并不感兴趣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c543f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果页面都是相似的（它们基本上都是相同类型的内容），你可能需要在现有的网页对象中加入一个 pageType 属性\n",
    "class Website:\n",
    "    \"\"\"所有文章/网页的共同基类\"\"\"\n",
    "    \n",
    "    def __init__(self, type, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        self.pageType = pageType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a1f4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果你抓取的页面或内容各不相同（它们包含不同类型的字段），就需要为每个页面类型创建一个新的对象\n",
    "class Website:\n",
    "    \"\"\"所有文章/网页的共同基类\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "\n",
    "#这不是一个由你的爬虫直接使用的对象，而是将被你的页面类型引用的对象\n",
    "class Product(Website):\n",
    "    \"\"\"产品页面要抓取的信息\"\"\"\n",
    "    def __init__(self, name, url, titleTag, productNumber, price):\n",
    "        Website.__init__(self, name, url, TitleTag)\n",
    "        self.productNumberTag = productNumberTag\n",
    "        self.priceTag = priceTag\n",
    "        \n",
    "class Article(Website):\n",
    "    \"\"\"文章页面要抓取的信息\"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag, dateTag):\n",
    "        Website.__init__(self, name, url, titleTag)\n",
    "        self.bodyTag = bodyTag\n",
    "        self.dateTag = dateTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3a3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
